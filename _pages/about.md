---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

* I am a final year [Informatics](https://ist.psu.edu/) Ph.D. student at [Pennsylvania state university](https://www.psu.edu/), advised by Dr. [Lee Giles](https://clgiles.ist.psu.edu/). I closely work and collaborate with Dr. [Alexander G. Ororbia II](https://www.cs.rit.edu/~ago/) and Dr. [Daniel Kifer](http://www.cse.psu.edu/~duk17/). I am a lab member of both the Intelligent Information Systems ([IIS](http://iis.ist.psu.edu/)) and the Neural Adaptive Computing ([NAC](https://www.cs.rit.edu/~ago/nac_lab.html)) laboratories. [[Curriculum Vitae](http://ankurmali.github.io/files/Curriculum_Vitae.pdf)] [[Google Scholar](https://scholar.google.co.in/citations?user=ogxlzgcAAAAJ&hl=en)].
* My work is at the intersection of language, memory, and computationâ€”spanning Natural Language Processing (NLP), linguistics, and formal language theory. In particular, I have proposed several knowledge-guided interpretable deep learning systems focusing on generating trustworthy information. Furthermore, I have also designed approaches to investigate the mysterious success of deep learning in recognizing natural language from a theoretical and empirical perspective.
  * I have designed a theoretical model as well as an empirical model in Tensor recurrent models that are equivalent to pushdown automata and are also Turing complete with finite precision and weights.
  * I have also worked on extracting state abstraction/representation from these networks for model verification.
* I also work on neural compression (image/video) and model compression (introducing sparsity/compression while training) systems.
  * I have designed hybrid approaches that take components from classical compression techniques (JPEG/JPEG 200) and combine them with neural networks to produce computationally efficient models that are comparable to DNNs based compression systems.
* I also work on designing learning algorithms and computational architectures guided by theories of the brain and its functionality that emphasize solving challenges such as continual/lifelong learning, learning with minimal supervision, RL, and sparsity (both in computer vision and natural language processing).
  * I develop local learning algorithms (derivative-free, LRA, LRA-E, DTP-Sigma) that are biologically inspired and forward propagation (a variant of RTRL and UORO), that serve as an alternative to BP and BPTT to train deep neural systems and RL agents.

# I am on the job market (academic/Research). If you have any openings related to Interpretable ML/Lifelong learning ML, I'd love to get in contact!
        
# Recent News
* Dec 2021: One paper accepted in AAAI-22 (oral) (Backprop free RL) and another paper is accepted in DCC-22 (Neural JPEG).
* July 7th, 2021:Investigating backpropagation alternatives when learning to dynamically count with recurrent neural networks accepted as oral publication in  15th International Conference on Grammatical Inference (ICGI-2021) published at Proceedings of Machine Learning Research (PMLR).
* July 7th, 2021: Recognizing long grammatical sequences using recurrent networks augmented with an external differentiable stack accepted as oral publication in  15th International Conference on Grammatical Inference (ICGI-2021) published at Proceedings of Machine Learning Research (PMLR).
* May 20th, 2021:Omnilayout: Room layout reconstruction from indoor spherical panoramas accepted as a poster at CVPR workshop 2021.
* Jan 16, 2021: I am a reviewer for the following top conferences and journals AAAI, ICLR, CVPR, ICCV/ECCV, ICML, ACL, EMNLP, Frontier, IEEE TNNLS, Neco.
* Jan 14, 2021: Will be joining Nvidia Research as Research Intern for Summer-21
* Dec 31, 2020: The Neural State PushDown Automaton accepted as a full article in IEEE Transactions on Artificial Intelligence (TAI 2021).
* Dec 27, 2020: An Empirical Analysis of Recurrent Learning Algorithms In Neural Lossy Image Compression Systems accepted as a poster for publication at the 2021 Data Compression Conference proceedings (DCC 2021).
* Dec 1, 2020: Recognizing and Verifying Mathematical Equations using Multiplicative Differential Neural Unit, accepted for publication at the thirty-fifth AAAI Conference on Artificial Intelligence (AAAI-21).
* May 11, 2020: Started my internship at Nvidia Research, Learning and Perception Research team on Continual Learning (Summer-20). 
* April 17, 2020: Successfully defended comprehensive exam (Now ABD).
* June 25, 2019: Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations accepted as a full article in IEEE Transactions on Neural Networks and Learning Systems (TNNLS 2019).
* Dec 27, 2019: Sibling Neural Estimators: Improving Iterative Image Decoding with Gradient Communication accepted as a full paper for publication at the 2019 Data Compression Conference proceedings (DCC 2020).
* May 17, 2019: Successfully cleared Candidacy. 
* May 14, 2019: Like a Baby: Visually Situated Neural Language Acquisition accepted for publication at the 57th Annual Meeting of the Association for Computational Linguistics conference (ACL 2019) (Work with Google AI).
* March 11, 2019: A Neural Temporal Model for Human Motion Prediction accepted for publication at the Conference on Computer Vision and Pattern Recognition (CVPR 2019).
* December 27, 2018: Learned Neural Iterative Decoding for Lossy Image Compression Systems accepted as a full paper for publication at the 2019 Data Compression Conference proceedings (DCC 2019).
* October 31, 2018: Biologically Motivated Algorithms for Propagating Local Target Representations, accepted for publication at the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19). 
