---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

* I am a final year [Informatics](https://ist.psu.edu/) Ph.D. student at [Pennsylvania state university](https://www.psu.edu/), advised by Dr. [Lee Giles](https://clgiles.ist.psu.edu/) and mentored by Dr. [Alexander G. Ororbia II](https://www.cs.rit.edu/~ago/) and Dr. [Daniel Kifer](http://www.cse.psu.edu/~duk17/). I am a lab member of both the Intelligent Information Systems ([IIS](http://iis.ist.psu.edu/)) and the Neural Adaptive Computing ([NAC](https://www.cs.rit.edu/~ago/nac_lab.html)) laboratories. [[Curriculum Vitae](http://ankurmali.github.io/files/Curriculum_Vitae.pdf)] [[Google Scholar](https://scholar.google.co.in/citations?user=ogxlzgcAAAAJ&hl=en)].
* My work is at the intersection of language, memory, and computationâ€”spanning Natural Language Processing (NLP), linguistics, and formal language theory. In particular, I have proposed several knowledge-guided interpretable deep learning systems focusing on generating trustworthy information. Furthermore, I have also designed approaches to investigate the mysterious success of deep learning in recognizing natural language from a theoretical and empirical perspective.
  * I have design theoretical model as well as empricial model in Tensor recurrent models that are equivalent to pushdown automata and are also turing complete with finite precision and weights.
  * I have also worked on extracting on state abstraction from these networks for model verification.
* I also work on neural compression (image/video) and model compression (introducing sparsity/compression while training) systems.
  * I have designed hybrid approaches that take components from classicial compression techniques (JPEG/JPEG 200) and combine them with neural networks to produce computationally efficient models that are comparable to DNNs based compression systems.
* I also work on designing learning algorithms and computational architectures guided by theories of the brain and its functionality that emphasize solving challenges such as continual/lifelong learning, learning with minimal supervision, RL and sparsity (both in computer vision and natural language processing).
  * I develop local learning algorithms (derivative free, LRA, LRA-E, DTP-Sigma) that are biologically inspired and forward propagation (variant of RTRL and UORO), that serve as alternative to BP and BPTT to train deep neural systems and RL agents.

# I am on the job market (academic/Research). If you have any openings related to Interpretable ML/Lifelong learning ML, I'd love to get in contact!
        
# Recent News
* Dec 2021: One paper accepted in AAAI-22 (oral) (Backprop free RL) and another paper is accepted in DCC-22 (Neural JPEG).
* July 7th, 2021:Investigating backpropagation alternatives when learning to dynamically count with recurrent neural networks accepted as oral publication in  15th International Conference on Grammatical Inference (ICGI-2021) published at Proceedings of Machine Learning Research (PMLR).
* July 7th, 2021: Recognizing long grammatical sequences using recurrent networks augmented with an external differentiable stack accepted as oral publication in  15th International Conference on Grammatical Inference (ICGI-2021) published at Proceedings of Machine Learning Research (PMLR).
* May 20th, 2021:Omnilayout: Room layout reconstruction from indoor spherical panoramas accepted as poster at CVPR workshop 2021.
* Jan 16, 2021: I am reviewer for following top conferences and journal AAAI, ICLR, CVPR, ICCV/ECCV, ICML, ACL, EMNLP, Frontier, IEEE TNNLS, Neco.
* Jan 14, 2021: Will be joining Nvidia Research as Research Intern for Summer-21
* Dec 31, 2020: The Neural State PushDown Automaton accepted as full article in IEEE Transactions on Artificial Intelligence (TAI 2021).
* Dec 27,2020: An Empirical Analysis of Recurrent Learning Algorithms In Neural Lossy Image Compression Systems accepted as poster for publication at the 2021 Data Compression Conference proceedings (DCC 2021).
* Dec 1, 2020: Recognizing and Verifying Mathematical Equations using Multiplicative Differential Neural Unit, accepted for publication at the thirty-fifth AAAI Conference on Artificial Intelligence (AAAI-21) .
* May 11, 2020: Started my internship at Nvidia Research, Learning and Perception Research team on Continual Learning (Summer-20). 
* April 17, 2020: Successfully defended comprehensive exam (Now ABD).
* June 25, 2019: Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations accepted as full article in IEEE Transactions on Neural Networks and Learning Systems (TNNLS 2019).
* Dec 27,2019: Sibling Neural Estimators: Improving Iterative Image Decoding with Gradient Communication accepted as full paper for publication at the 2019 Data Compression Conference proceedings (DCC 2020).
* May 17, 2019: Successfully cleared Candidacy. 
* May 14, 2019: Like a Baby: Visually Situated Neural Language Acquisition accepted for publication at the 57th Annual Meeting of the Association for Computational Linguistics conference (ACL 2019) (Work with Google AI).
* March 11, 2019: A Neural Temporal Model for Human Motion Prediction accepted for publication at the Conference on Computer Vision and Pattern Recognition (CVPR 2019).
* December 27, 2018: Learned Neural Iterative Decoding for Lossy Image Compression Systems accepted as full paper for publication at the 2019 Data Compression Conference proceedings (DCC 2019).
* October 31, 2018: Biologically Motivated Algorithms for Propagating Local Target Representations, accepted for publication at the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19). 
